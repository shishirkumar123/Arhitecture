Kafka Architecture
=====================
Kafka Producer, Broker, Consumer are designed to run on different JVMs (though technically they can run of same m/c)
All of them can be scaled horizontally.
They communicate using Kafka binary protocol over TCP. This protocol allows for compression of messages (e.g., GZIP, Snappy), further reducing the amount of data transmitted over the network.
Kafkaâ€™s efficiency stems from its event-driven architecture and reliance on well-optimized loops to handle network I/O, log management, and replication
Kafka uses asynchronous I/O (via Java NIO) for efficient communication between clients and brokers.
Zookeeper is(was) needed for coordination among the kafka brokers.
Starting from Kafka 2.8, Kafka introduced KRaft to handle the responsibilities previously managed by ZooKeeper, such as:
	Metadata management.
	Leader election.
	Configuration storage.

Running console based Kafka 
===========================
.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties
.\bin\windows\kafka-server-start.bat .\config\server.properties
.\bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic promotion
.\bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic promotion
.\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic promotion --from-beginning
----
List of Topics -
.\bin\windows\kafka-topics.bat --list --zookeeper localhost:2181
Delete a Topic
.\bin\windows\kafka-topics.bat --delete --zookeeper localhost:2181 --topic test [work only if delete.topic.enable is set to true]
[This only marks for deletion. When does actual deletion happens? If you fire 'describe' on the deleted Topic, you may get some info with MarkedForDeletion:true]
Describe a Topic -
.\bin\windows\kafka-topics.bat --describe --zookeeper localhost:2181 --topic test

Create partition during topic creation
.\bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 5 --topic promotion1

Consume from a topic specifying group
.\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic promotion1 --group 1

Consume from a topic specifying partition and offset
.\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic promotion1 --partition 0 --offset 61
[if specifying offset, you must specify partition]

Note that partition is not specified during publishing. Once a producer start publishing, the content goes evenly to all partitions.
[Observation - For all the message that I published, one message got occupied in one offset] 

Q1. What will happen if you have only one broker, but you are trying to create topic with replication factor 3?
A1.org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 3 larger than available brokers: 1.
Q2. How does Kafka ensure that the message is consumed only once?
A2. By setting appropriate levels of Delivery Guarantees-
	At-Least-Once Delivery (default)
	At-Most-Once Delivery
	Exactly-Once Semantics (EOS)
Q3. How does consumer decide from which replication to read? Leader's copy?
A3. Yes. Consumers and producers interact with leader only.

======================
Theory
--------
Kafka Server = Kafka Broker. Ideally 1 brocker on 1 m/c, but technically possible to have n broker(with different port) on 1 m/c
Kafka Cluster - Multiple Kafka Servers co-ordinates through Zookeeper.
Topic can be divided into multiple Partition
Each Partition can exist on different machine.
A partition can not not not be divided further into multiple machine. 
Each Partition has offsets. 
Q. How big is one offset?
A. Kafka allows individual messages to be up to 1 MB by default. You can change settings-
	At broker: message.max.bytes=<desired size in bytes>
	At producer side: max.request.size=<desired size in bytes>
Compression and batching can effectively increase the amount of data associated with each offset.
Offset number is not globally unique, but unique in partition.
So, to locate a meesage i need to know topic, partition number, offset number.

Q. what will happen of producer sent a message whose size is bigger that what 1 offset can take?
A. org.apache.kafka.common.errors.RecordTooLargeException: The message is larger than the maximum allowed size.

My notes -In this scenario, retry doesnt make sense. This is not n/w issue. So while creating our own retry logic,
all these scenario needs to be considered. Better use out of the box retry logic.

No of partition = no of consumers in a consumer group. Consumer can't be more than number of partition- to avoid double read. But can be less. In that case one consumer is reading from more than 1 partion. See kafka_overview.png
If no of consumer >  no of partition, some consumer will have to sit idle.
If there are 4 brockers and I need 4 partitions each brocker can have one partition.
But if I have one brocker and I need 2 partition, both partition will reside on same brocker.
If you have a single broker and create a topic with 4 partitions:
All 4 partitions will reside on the same broker.
When additional brokers are added, partitions can be rebalanced across the cluster to improve fault tolerance and load distribution.

Replication factor is associated with Topic.
For a given Topic, if replication factor is say 3, then one broker is nominated a leader for each partition. Only leader talks to Producer and Consumer for that Topic and sends acknowledgement to producer. 
Leader maintains first copy of replication, followers maintain rest.
Q. Is it possible that I have 3 partitions on 3 brockers... partition 1 leader is broker 2, but partition 2 leader is broker 3?
A. Yes.
Q. In other words, leader is at partition level or topic level?
A. partition level.
Use command kafka-topic describe to see these details. It shows leader of each partition. It also shows replica ids and ISR ids.
(ISR - In Sync Replicas)
Sending message requires properties - bootstrap.servers, serializer(to convert object into byte stream)
Q. When a message is sent to kafka cluster, how is it decided that which partition the message will go?
A. 	Key Provided: Partition determined using a hash of the key.
	No Key Provided: Round-robin or default partitioner used.
	Custom Logic: Custom partitioner can override defaults.
	Manual Partitioning: Producer specifies a partition explicitly.

PRODUCER SIDE
================================================================
3 approach for sending - fire and forget, wait till acknowledgement(Future.get), async.
-------
Fire and forget - Useful when low latency/high throughput is crucial, and the application can tolerate potential data loss.
ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
producer.send(record);  // No callback or acknowledgement, Fire and Forget
--------
Wait till acknowledgement(Future.get) - producer can wait for the acknowledgment by calling Future.get(), which blocks 
until the message is successfully written to the partition (either leader or replicated) or a failure occurs.
ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
Future<RecordMetadata> future = producer.send(record);
try {
    RecordMetadata<BusinessEntity> metadata = future.get();  // Wait for acknowledgement
    System.out.println("Message sent to partition " + metadata.partition());
} catch (Exception e) {
    e.printStackTrace();
}
--------
async - The producer sends a message to the Kafka broker.
A callback function is invoked once the broker acknowledges the message or if an error occurs.
This allows the producer to continue sending other messages without waiting for the response, 
but still handle success/failure asynchronously.

ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
producer.send(record, new Callback() {
    @Override
    public void onCompletion(RecordMetadata<BusinessEntity> metadata, Exception exception) {
        if (exception != null) {
            // Handle failure (e.g., log or retry)
            exception.printStackTrace();
        } else {
            // Handle success (e.g., log metadata)
            System.out.println("Message sent to partition " + metadata.partition());
        }
    }
});


In the record which is send, there is a key parameter. The key is a powerful feature in Kafka that 
allows you to control partitioning, ensure message ordering, and enable efficient data grouping
(e.g., by user, account, or region) for processing.

===================================================================================
Serialization - 
Kafka provides StringSerializer,IntSerialzer,LongSerializer, DoubleSerializer,StringSerialiser etc.
If we are sending business objects like Order, Customer, Supplier, we need to create custom serializer.

Use default or Custom. When message object changes, need to update custom serializer accordingly. 
	But then old message cant be read with new Serializer. Avro helps in these situations.

====================================================================================
If order is important then there are two ways to achieve it-
	1. Use Synchronous send
	2. set max.in.flight.request.per.connection = 1
	Of course, then guaranty comes at the cost of throughput.


Common Configurations
--------------
broker.id - Unique identifier of broker.
port - Port on which broker runs.
log.dirs - the path where logs of broker will be written.

zookeeper.connect - using this broker knows where is zookeeper running.
delete.topic.enable
autocreate.topic.enable
default.replication.factor
num.partition
log.retention.ms - How long to keep message(Default 7 days)
log.retention.bytes - Max capacity of size of message a partition will hold. Beyond that, Kafka will do the cleanup.

Producer specific configuration
--------------------------------
1. ack - Three possible values
	0 - fire and forget, (lowest latency)
	1 - Leader receives the message and sends the acknowledgement. 
		What if leader crashed before replicating to follower? 
		Producer will get the acknowledgement but message is lost. So next option 'All' is better.
	All - Acknowledgement is required from broker to producer. If set to all, then the leader broker will confirm 
		from all the follower broker whether message was successfully received, then only send the
		acknowledgement message.(slowest)

Q. Scenario: In case of ack=All, Leader received the message, but error occurred while replicating to the followers.
So acknowledgement is not sent. So the producer resends the message. Leader again receives the same message. 
Will this cause 2 times consumption of the message by the consumer?
A: Kafka's idempotent producer ensures that even if the same message is retried multiple times due to lack of acknowledgment, it is written to the topic only once. This is achieved using a producer ID (PID) and sequence numbers. When the leader receives a duplicate message (same PID and sequence number), it discards it as a duplicate.

2. retries - How many times a producer will retry when error occured. Default - 0 (zero)
[Related configuration-
	retry.backoff.ms - specifies the amount of time (in milliseconds) a producer or consumer should wait 
		   before retrying a failed operation. Default - 100
]
3. max.in.flight.request.per.connection - how many in flight request is allowed that are still not acknowledged.
Setting high value will give high throughput at the expense of high memory consumption.

Look into more configurations which are interesting
buffer.memory
compression.type
batch.size
linger.ms
client.id
max.request.size

-------------------------
CONSUMER SIDE
================================================================

Consumer - There is a construct consumer group which are set of related consumers working together to achieve high throughput. 
	A consumer is assigned to a partition. For a given consumer group, only single consumer can be assigned to partition.

Number of partition is upper limit of consumers you can have in a group. Kafka will allow it, but extra consumers will be doing nothing.

Group co-ordinator and Leader.
----------------------------------------
Group Coordinator:
------------------
This is a Kafka broker, not a consumer.
The Group Coordinator is responsible for managing the consumer group.
It keeps track of the group membership, handles heartbeats, and initiates rebalances when necessary.

Consumer Group Leader:
----------------------
Within the consumer group, one consumer is elected as the group leader during the group formation or rebalance process.
The leader consumer communicates with the Group Coordinator on the broker to obtain partition assignments for all members of the group.
After determining the assignments, the leader informs all other consumers about their respective partitions.
Rebalancing is initiated by Group coordinator but executed by Leader.

Remember - 
The Group Coordinator is always a Kafka broker.
The Group Leader is one of the consumers in the group.
-----------------------------------------------------------------------
What does this code do: kafkaConsumer.poll(100)
		- connect to group coordinator
		- join the group
		- receives the partition assignment
		- sends heartbeat
		- fetch your message
		- many more things...
		But the API (pll) is very simple and clean.
	If a consumer hasn't poll for a while, group co-ordinator may assume, it is dead and trigger a partition rebalance.

------------------------------------------------------------------------
	One consumer can subscribe to list of topics.
	Consumer polls the topic in infinite loop. Optionally it can be given a scheduler so that it pools based on scheduler settings.
	

	Q. Why Kafka choose consumer polling approach. Why it didn't choose broker notifying consumer approach?
	
Offset
	Current Offset - Maintains current position of consumer read.
	Committed offset - Consumer has confirmed about processing.
	These offsets play a role in rebalancing
-------------------------------------------------------------------
Note: Make sure all the prop configurations are in separate file, not mixed with code.
--------------------------------------------------------------------------------------
Offset
	One message goes to one offset. Message can't be splitted to multiple offset.
  Two offsets for each consumer
	Current: (Sent records). After each call to consumer.poll, current offset jumps to new position depending on how many message read is configured. 
		Used to avoid resending same message to same consumer.
	Commited: (Processed records). The offset position that is processed by consumer. (Obviously this offset will be behind current offset.)
		Used in the event of rebalance to resending same message to the new consumer which is already processed by previous consumer.

Coomitted offset plays a criticle role during rebalancing. New member will need to know where to start. The answer is committed offset.

How to commit. Two ways:
	Auto commit
	Manual commit
		Two ways of mannual commit:
		-Commit Sync - reliable, strightfoward but blocking. Retry for recoverable errors.
		-Commit Async - no retry. Why it is designed like that?
			e.g consumer needs to processed 75 records. it failed. Consumer will try after 5 seconds.
			Because it is async, without knowing previous 75 records failure, new 100 records are commited.
			This situation can have undesired result. Thats why commit async has been designed to have no retry.
	Controlled through below properties:
		enable.auto.commit - default is true
		auto.commit.interval.ms - default is 5 seconds
	Q. Are the above property ( 5 seconds in the example) same for all consumer? Or can be set differently for different consumer?
	My notes: Why can simply autocommit after each message is processed?  Why to wait for a time interval. 
		  Sometime it may be become less than required or sometime it can become more than required.


Q. On kafka.poll call, how many messages are read?
A. When calling kafka.poll, the number of messages read is controlled by the consumer configuration setting "max.poll.records", which specifies the maximum number of 
records a single poll operation will return; by default, this value is set to 500 messages. So after each call, current offset jumps by that many number 
(500 in this case)
Higher the number, more performant it can be but memory requirement will increase.

---------------------------------------------------------------------------------------

RebalanceListener

Partition assignment can be done by Kafka or us. If we need to do it, implement ConsumerRebalanceListener. Methods
	onPartitionRevoked - we can commit the current offset just before partition is revoked from a consumer.
	onPartitionAssigned - called just after rebalancing is complete, consumer starts consuming.

--------------------------------------------------------------------------------
Custom partition assignment  + automatic partition assignment = problem.
(Remember: custom partition happing at producer side. Auto partition assignment happening at consumer side.)

Problem 1: Imbalance in Consumer Workload lead to performance bottlenecks.
If the producer uses custom partitioning to send data unevenly to partitions (e.g., 90% of messages to Partition 1 
and only 10% to Partition 2), the consumers in the group may not have balanced workloads.


----------------------------------------------------------------------------------

Kerberos security
Serializer problem
Message encryption



