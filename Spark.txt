Architecture
=============
Spark Jobs - The work, that needs to be done. Jobs can be submitted in two modes:
    Yarn client mode
    Cluster mode
Tasks: Jobs are devided into tasks?
Driver: Driver submits the jobs to the cluster Manager. 
Executor- Running Java process
Executor Core- Running Java threads.Executor runs parallel executor core.

Compare to Hadoop: Spark is faster for many reasons
    Iteration Algo(Iteration iteration input is previous iteration output): 
        Spark use lineage in RAM, Hadoop do disk i/o after each iteration.
    Executor of Spark is reused. So no JVM start/stop
    Spark has caching capabilities



RDD, DataSet, DataFrame
========================
RDD: Resilient Distribute ...
    Immutable
    Fault tolerent, typesafe.
    Developer needs to take care of optimization
    Not at good as DataSet in performance
    Not memory efficient
    Joining is not easy
    Connection with Hive isn't easy

Dataframe
    Table abstraction at the top of RDD.
    Inbuild Hive connection
    Auto optimization using Catalist Optimizer
    Performance is better because an execution plan is created before running.
    Not as good as DataSet
    Not type Safe(to achieve optimization)

DataSet
    Type Safe, Autooptimized, Better performance.
    More Memory efficient.
    Encoders - New way of serialize/deserialize.
=======================
Partition vs Bucketing
    Done during write operation for optimization during read. Done by separating the data on some basis then process parallely.
    Use Case: Huge dataset has three columns - name, age, location.
    Solution: Do partitioning by location, then for each partition, do the bucketing by age.
    So total number of files = Number of partition * number of buckets.
    inside the bucket, keep the file format as parquet/OCR to further optimize the read operation.
    Note: Buckets can be created without partitions as well.

My notes: In the above example, after partition two situations-
1. Search based on partition column:
   e.g Need to find maz(age), min(age), avg(age) in Florida. -This will be faster because need to search in Florida partition only.
2. Search based on column other tha partition column:
    e.g Need to find person with name 'John'. Need to search every partition, but still faster because of parallelism.

Bucket in Spark vs Hive:
-------------------------
Hive will shuffle the data during Reduce phase. (Number of reducers = number of buckets.
Spark doesn't use reduce phase. Everything is done in map. No shuffling.

Caculating number of buckets
------------------------------
In HDFS, default size of block is 128 MB. 
So no. of buckets = Size of data in mb/128

Prefer bucketing over partitioning specially to avoid so many small files.
=======================
Repartition vs Coalesce
    When partition size is uneven, some tasks will finish sooner than the otherSo overall time taken will be high
    Repartition and Coalesce rearrage the partition size.
    Repartition do it across machines (too much shuffling), coalesce do it within machine. So coalesce is faster.

=======================

Choice of language
    Scala: Better enterprise acceptance because of familiarity with Java, Type safe, UDFs are better in performance.
    Python: Easier, better ML library(Pandas, Tensorflow, Scifi), visualization library is better. Ecosystem has more contributer.

Choice of file format
=========================
Avro: Row based format. Needed when you need complete row fetch(all/most of the columns.
Parque: Column based format. Needed when you are interested in only few columns. Write optimized
    Parque format
        RowData
            TODO
            TODO



    
